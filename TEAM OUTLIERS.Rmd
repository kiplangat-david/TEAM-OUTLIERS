---
title: "APARTMENT PRICES"
author: "TEAM OUTLIERS"
date: "2/7/2022"
output: html_document
---

```{r}
library(tidyverse)
library(tibbletime)
library(anomalize)
library(timetk)
library(ggplot2)
library(data.table)
library(corrplot)
library(caret)
library(rpart)
library(gmodels)
library(ggmosaic)
library(rpart.plot)
library(fpc)
library(NbClust)
library(flextable)
library(dplyr)
library(cluster)
library(factoextra) #To create a beautiful graph of the clusters    generated with the kmeans() function
options(warn=-1) #turn off warnings
```

```{r}
df <- read.csv('C:/Users/WILLY KIRUI/Contacts/Documents/Daegu_Real_Estate_data.csv')
```


```{r}
head(df)
```


```{r}
sum(is.na(df))
```

```{r}
str(df)
```
```{r}
#check summary of the sales variable
summary(df$SalePrice)
```


```{r}
#Distribution of saleprice
ggplot(df,aes(x = SalePrice) )+ geom_histogram(bins = 30, fill= 'darkblue')
```

```{r}
CrossTable(df$HallwayType)

```

```{r}
#check distribution of hallwaytype
colors <- c('darkblue', 'royalblue', 'darkred')

hallwaytype <- table(df$HallwayType)

barplot(hallwaytype, col = colors,
        main = "HallwayType")
```

```{r}
CrossTable(df$HeatingType)
```

```{r}
#check distribution of HeatingType
colors <- c('darkblue', 'royalblue', 'darkred')

heatingtype <- table(df$HeatingTypec)

barplot(heatingtype, col = colors,
        main = "HeatingType")
```

```{r}
CrossTable(df$AptManageType)
```

```{r}
#check distribution of AptManageType
colors <- c('darkblue', 'royalblue', 'darkred')

aptmanagetype <- table(df$AptManageType)

barplot(aptmanagetype, col = colors,
        main = "AptManageType")
```

```{r}
ggplot(df, aes(x = YearBuilt, y = SalePrice)) +
  geom_point()+
  stat_smooth(method = 'lm')
```

```{r}
ggplot(df, aes(x = Floor, y = SalePrice)) +
  geom_point() +
  stat_smooth(method = 'lm')
```

```{r}
ggplot(df, aes(x = AptManageType, y = SalePrice)) +
  geom_boxplot() 
```

```{r}

 ggplot(df, aes(x = HeatingType, y = SalePrice)) + geom_boxplot()+
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
```

```{r}
p1 <- ggplot(df, aes(x = AptManageType, y = SalePrice)) + geom_point()+
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
p2 <- ggplot(df, aes(x = HeatingType, y = SalePrice)) + geom_jitter()+
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
p3 <- ggplot(df, aes(x = HallwayType, y = SalePrice)) + geom_boxplot()+
    theme(axis.text.x = element_text(angle = 30, hjust = 1))

ggarrange(p1, p2, p3, ncol=2)
```

```{r}
#select list of categorical variables,df1
df1 <- df[,c(7,8,9,12,13,17)]#categorical
#make a subset of non-categorical variables,df2
df2 <- df[,c(1,2,3,4,5,6,10,11,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30)]
```

```{r}
head(df1)
```


```{r}
dmy <- dummyVars(" ~ .", data = df1, fullRank = T)
df3 <- data.frame(predict(dmy, newdata = df1))
```

```{r}
head(df3)
```

```{r}
data <- cbind(df3,df2)
```

```{r}
head(data)
```

```{r}
# Calculating the correlation matrix
# ---
#
correlationMatrix <- cor(data)

# Find attributes that are highly correlated
# ---
#
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)

# Highly correlated attributes
# ---
# 
highlyCorrelated

names(data[,highlyCorrelated])
```

```{r}
# Removing Redundant Features 
# ---
# 
df<-data[-highlyCorrelated]

# Performing our graphical comparison
# ---
# 
par(mfrow = c(1, 2))
corrplot(correlationMatrix, order = "hclust")
corrplot(cor(df), order = "hclust")
```

```{r}
df <- scale(df)
```


##creating train test sets
```{r}
set.seed(387)
train_data = createDataPartition(y = df$SalePrice, p = 0.70, list = FALSE)
test_data =createDataPartition(y=df$SalePrice, p=0.30,list=FALSE)
train = df[train_data, ]
test = df[test_data, ]
```


#model building
#linear regression
```{r}
model = lm( SalePrice ~., data = train)
```

##summary of the model
```{r}
summary(model) # Obtain coefficients, Residuals and statistics
rsquare = summary(model)$r.squared # R-squared value
```
##predictions
```{r}
predictions = predict(model, newdata = test)
predicted.vs.original = data.frame(predicted = predictions, original = test$SalePrice)   # Create a new data frame
ggplot(predicted.vs.original, aes(x = predicted, y = original)) +
 geom_point() +
 geom_smooth(color='blue') +
 labs(x = 'Predicted Values', y = 'Original Values', title = 'Predicted vs. Original Values') +
 theme_minimal()
```

```{r}
install.packages('xgboost')
library(xgboost)
set.seed(1234)

cvcontrol <- trainControl(method = "repeatedcv",
                          number = 5,
                          repeats = 2,
                          allowParallel = TRUE)





set.seed(1978)
boosting1<- train(SalePrice ~., data = training_set,
                  method = "xgbTree",
                  trControl = cvcontrol,
                  tuneGrid = expand.grid(nrounds = 720,
                                         max_depth = 7,
                                         eta = 0.25,
                                         gamma = 2.2,
                                         colsample_bytree =1,
                                         min_child_weight = 1,
                                         subsample = 1))
```

```{r}

```

```{r}

```

```{r}

```

